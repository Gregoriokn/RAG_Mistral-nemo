{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bm-pQgz3dgQr"
   },
   "outputs": [],
   "source": [
    "\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using llama3 and nomic-embed-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "S3Pw_eekOpuW",
    "outputId": "1d9e7b44-ed68-4148-b164-f686bd14106f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n",
      "\u001b[?25l\u001b[?25lpulling manifest ⠋ \u001b[?25hpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25hpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25hpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[2K\u001b[1Gpulling manifest ⠴ pulling manifest ⠴ \u001b[?25h\u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[2K\u001b[1Gpulling manifest ⠦ pulling manifest ⠦ \u001b[?25h\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[?25lpulling manifest ⠧ \u001b[?25h\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[2K\u001b[1Gpulling manifest ⠇ pulling manifest ⠇ \u001b[?25h\u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25hpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling b559938ab7a0... 100% ▕████████████████▏ 7.1 GB                         \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling f023d1ce0e55... 100% ▕████████████████▏  688 B                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 43070e2d4e53... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         pulling ed11eda7790d... 100% ▕████████████████▏   30 B                         \n",
      "\n",
      "pulling 65d37de20e59... 100% ▕████████████████▏  486 B                         \n",
      "verifying sha256 digest \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "writing manifest verifying sha256 digest \n",
      "writing manifest \n",
      "\n",
      "success success \u001b[?25h\u001b[?25h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama serve & ollama pull mistral-nemo & ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ovlpFAQbPpYK"
   },
   "outputs": [],
   "source": [
    "!pip -qq install langchain\n",
    "!pip -qq install langchain-core\n",
    "!pip -qq install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      "Use Ctrl + d or /bye to exit.\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ollama run mistral-nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLNJWzd0Ul9V",
    "outputId": "2dae7d24-e1f3-401e-a4b8-a2db3bf9a6a6"
   },
   "outputs": [],
   "source": [
    "!pip install ollama langchain beautifulsoup4 chromadb gradio pymupdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregoriokoslinskineto/miniconda3/envs/mestrado/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import fitz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read text from a PDF\n",
    "def read_pdf(file_path):\n",
    "    pdf_text = \"\"\n",
    "    with fitz.open(file_path) as pdf:\n",
    "        for page in pdf:\n",
    "            pdf_text += page.get_text()\n",
    "    return pdf_text\n",
    "\n",
    "# Load the data from a PDF file\n",
    "pdf_path = \"krebs_casiopacheco_m.pdf\"  # Change to the path of your PDF\n",
    "pdf_content = read_pdf(pdf_path)\n",
    "\n",
    "\n",
    "# Split the loaded PDF content into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.create_documents([pdf_content])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the web URL\n",
    "url='https://pt.wikipedia.org/wiki/Resident_Evil_4'\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the loaded documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "DlJzBxznmQrw",
    "outputId": "e42062df-38d3-4ede-d97a-e464d0599e42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/0dsskf1n0ts_874pj62szfsm0000gn/T/ipykernel_10356/1797064749.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
     ]
    }
   ],
   "source": [
    "# Create Ollama embeddings and vector store\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Define the function to call the Ollama Mistral-nemo model\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='mistral-nemo', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Define the RAG setup\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Gradio interface\n",
    "def get_important_facts(question):\n",
    "    return rag_chain(question)\n",
    "\n",
    "# Create a Gradio app interface\n",
    "iface = gr.Interface(\n",
    "  fn=get_important_facts,\n",
    "  inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "  outputs=\"text\",\n",
    "  title=\"RAG with Mistral-nemo\",\n",
    "  description=\"Ask questions about the proveded context\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mestrado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
